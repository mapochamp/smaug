#include <google/protobuf/stubs/hash.h>
#include <iostream>

#include "smaug/core/static_graph_analyzer.h"
#include "smaug/utility/debug_stream.h"

namespace smaug {

auto print_pin = [](auto &node) {
    std::cout << "OPERATOR: [" << node.first->getName() << "] = " << std::endl;
    std::vector<TensorBase*> pin_vec = node.second;
    for(TensorBase* tensor : pin_vec) {
        std::cout << tensor->getName() << std::endl;
    }
};

void GraphAnalyzer::compare_schedule_list()
{
    std::cout << "======================================================\n";
    std::cout << "      Analyzing network graph...\n";
    std::cout << "======================================================\n";

    for (auto nameOp : network->getOperators()) {
        Operator* op = nameOp.second;
        dout(0) << "Tiling " << op->getName() << " ("
                << OpType_Name(op->getOpType()) << ").\n";
        op->tile();
        network_build_queue.push_back(op);
    }

    for (auto nameOp : network->getOperators()) {
        Operator* op = nameOp.second;
        Vertex vertex = op->getVertex();
        int numPendingInputs = boost::in_degree(vertex, network->getGraph());
        op->setNumPendingInputs(numPendingInputs);
        if (numPendingInputs == 0)
            readyQueue.push_back(op);
    }

    scheduleReady();

    std::cout << "======================================================\n";
    std::cout << "      Dumping network build queue...\n";
    std::cout << "======================================================\n";

    print_ops(network_build_queue);

    std::cout << "======================================================\n";
    std::cout << "      Analyzing network graph...\n";
    std::cout << "======================================================\n";

    print_ops(readyQueue);
}

void GraphAnalyzer::print_ops(std::list<Operator*> queue)
{
    for(auto op : queue) {
        std::cout << "TENSOR NAME: " << op->getName() << std::endl;
        auto tensor_vector = op->getInputs();
        std::cout << "TENSOR INPUTS: " << std::endl;
        for(auto tensor : tensor_vector) {
            std::cout << tensor->getName() << std::endl;
        }

        std::cout << "TENSOR OUTPUTS: " << std::endl;
        tensor_vector = op->getOutputs();
        for(auto tensor : tensor_vector) {
            std::cout << tensor->getName() << std::endl;
        }
    }
}

void GraphAnalyzer::maybeRunOperator(Operator* op) {
    if (op->isDead()) {
        for (auto output : op->getOutputs())
            output->setDead();
    }
}

auto print_node = [](auto &node) {
    std::cout << "[" << node.first << "] = " << node.second->get_access_time_string() << std::endl;
};

void GraphAnalyzer::update_liveness_map(TensorBase* tensor, uint32_t op_cycle)
{
    if(livenessMap.count(tensor)) {
        LivenessData* data = livenessMap.at(tensor);
        data->update_liveness(op_cycle);
    } else {
        livenessMap.insert_or_assign(tensor, new LivenessData(tensor->getName()));
        livenessMap.at(tensor)->update_liveness(op_cycle);
    }
}

void GraphAnalyzer::get_liveness_data()
{
    uint32_t op_cycle{};
    for(auto op : readyQueue) {
        std::vector<TensorBase*> inputs = op->getInputs();
        for(TensorBase* input : inputs) {
            std::cout << "UPDATING OP MAP " << op->getName() <<  " WITH INPUT : " << input->getName();
            std::cout << " AT CYCLE: " << op_cycle << std::endl;
            update_liveness_map(input, op_cycle);
        }

        // iteratate through outputs and add to liveness map
        std::vector<TensorBase*> outputs = op->getOutputs();
        for(TensorBase* output : outputs) {
            std::cout << "UPDATING OP MAP " << op->getName() << " WITH OUTPUT : " << output->getName();
            std::cout << " AT CYCLE: " << op_cycle << std::endl;;
            update_liveness_map(output, op_cycle);
        }

        op_cycle++;
    }

    this->remove_duplicate_cycles();
}

void GraphAnalyzer::remove_ttl_overlaps()
{
}

// sort all the tensors based on their FoM. std::map is sorted by default since
// its a red black tree
std::map<TensorBase*, double> GraphAnalyzer::sort(std::vector<TensorBase*> tensor_vec)
{
    std::map<TensorBase*, double> fom_map{};

    for(auto tensor : tensor_vec) {
        fom_map[tensor] = livenessMap[tensor]->get_fomd();
    }

    return fom_map;
}

void GraphAnalyzer::validate_pin()
{
    // The backend here is hardcoded. This not ideal and should
    // be changed to be passed in as a param.
    std::cout << __LINE__ << std::endl;
    int spad_size = SmvBackend::SpadSize();
    // we do spac_count - 1 since we don't want to write to the output spad
    std::cout << __LINE__ << std::endl;
    int total_spad_size = spad_size * (spad_count -1);
    // list of tensors that are not in the input list for an op
    std::cout << __LINE__ << std::endl;
    std::vector<TensorBase*> non_input_vector{};

    // for each operator in the pin map, we want to go through and check that
    // each pinCandidate in the pinMap will actually fit and is needed
    std::cout << __LINE__ << std::endl;

    for(const auto op : readyQueue) {
    std::cout << __LINE__ << std::endl;
        auto &pin_vec = tensorPinMap[op];
        std::cout << __LINE__ << std::endl;
        auto inputs = op->getInputs();
        std::cout << __LINE__ << std::endl;
        int input_tensors_size{};
        std::cout << __LINE__ << std::endl;
        int remaining_spad_space{};
        std::cout << __LINE__ << std::endl;
        auto tensor_size = [](auto tensor_vec) {
        std::cout << __LINE__ << std::endl;
            int size{};
            std::cout << __LINE__ << std::endl;
            for(auto i : tensor_vec) {
                std::cout << __LINE__ << std::endl;
                i->getShape().storageSize() * i->getDataTypeSize();
                std::cout << __LINE__ << std::endl;
            }
            std::cout << __LINE__ << std::endl;
            return size;
        };
        std::cout << __LINE__ << std::endl;
        input_tensors_size = tensor_size(inputs);
        std::cout << __LINE__ << std::endl;
        remaining_spad_space = total_spad_size - input_tensors_size;
        std::cout << __LINE__ << std::endl;

        // TODO: for each iteration of the readyQueue, keep track of the cycle
        // and reference the livenessMap to see if the tensor in the pinmap
        // is alive before the current op.
        for(auto tensor : pin_vec) {
            std::cout << __LINE__ << std::endl;
            int tensor_size = tensor->getShape().storageSize() * tensor->getDataTypeSize();
            // if the tensor is larger than the spads then don't pin
            std::cout << __LINE__ << std::endl;
            if(tensor_size > total_spad_size) {
                std::cout << __LINE__ << std::endl;
                std::remove(pin_vec.begin(), pin_vec.end(), tensor);
            }
            // if the tensor is not an input and larger then the remaining
            // available space if the inputs were pinned, don't pin
            std::cout << __LINE__ << std::endl;
            if(std::find(inputs.begin(), inputs.end(), tensor) == inputs.end()) {
                std::cout << __LINE__ << std::endl;
                non_input_vector.push_back(tensor);
                std::cout << __LINE__ << std::endl;
                if(tensor_size > remaining_spad_space) {
                    std::cout << __LINE__ << std::endl;
                    std::remove(pin_vec.begin(), pin_vec.end(), tensor);
                    std::cout << __LINE__ << std::endl;
                }
            }
        }

        // for all the tensors that are left as pin candidates, all of them may not
        // be able to coexist. So sort by FOM and temporal priority and start
        // pruning.
        std::cout << __LINE__ << std::endl;
        auto fom_map = this->sort(non_input_vector);
        std::cout << __LINE__ << std::endl;
        for(auto &node : tensorPinMap)
        {
            print_pin(node);
        }
        for(auto [key, val] : fom_map) {
            std::cout << __LINE__ << std::endl;
            std::cout << "about to validate: " << key->getName() << std::endl;
            int tensor_size = key->getShape().storageSize() * key->getDataTypeSize();
            std::cout << "remaining_spad_space: " << remaining_spad_space << std::endl;
            std::cout << "tensor: " << key->getName() << " storageSize: "  <<
                key->getShape().storageSize() << " DataTypeSize: " << 
                key->getDataTypeSize() << std::endl;
            std::cout << __LINE__ << std::endl;
            if(tensor_size >= remaining_spad_space) {
                std::cout << __LINE__ << std::endl;
                //pin_vec.erase(std::find(pin_vec.begin(), pin_vec.end(), key));
                if(auto pos = std::find(pin_vec.begin(), pin_vec.end(), key) != pin_vec.end()) {
                std::cout << __LINE__ << std::endl;
                    pin_vec.erase(pos);
                }
                //std::cout << __LINE__ << std::endl;
                //fom_map.erase(key);
            } else {
                std::cout << __LINE__ << std::endl;
                remaining_spad_space -= tensor_size;
                std::cout << __LINE__ << std::endl;
            }
        }


        //TODO:IMPORTANT !!!!
        /*==================================================================|
         * =================================================================|
         * =================================================================|
         * =================================================================|
         * ===================IMPORTANT=====================================|
         * =================================================================|
         * =================================================================|
         * =================================================================|
         * =================================================================|
         * =================================================================|
         * So basically, right now if we prune stuff, they might be pruned in a
         * wierd order where TTL's aren't crossing but shit has to get written
         * back, they might be "pinned" later so we have to go grab it again. So
         * we can either follow the ONSRAM thing and only keep shit that stays
         * alive and fits its entire liveness duration or we can do it our way
         * where we create some runtime shit to actually figure out how to do the
         * writebacks and repinning. But basically TTL overlap is an issue now.
         *
         * Start with the OnSRAM versio first and come back to do our way. Maybe
         * it'll be better if we figure out some cool algorithm to be less greedy
         * and minimize pin writebacks => so if something is being used alot but
         * gets written back bc its not an input for like 2/7 times its being used,
         * we should still pin it just becuase it's not gonna stay pinned its
         * entire TTL.
         */
    }

}

void GraphAnalyzer::create_pin_map()
{
    // iterate backwards through the list
    std::list<Operator*>::reverse_iterator prev_op = readyQueue.rbegin();;
    for(auto op = readyQueue.rbegin(); op != readyQueue.rend(); op++) {
        auto inputs = (*op)->getInputs();
        auto outputs = (*op)->getOutputs();
        std::cout << "Tensor pin map: updating op: " << (*op)->getName() << std::endl;
        for(const auto tensor : inputs) {
            tensorPinMap[*op].push_back(tensor);
            std::cout << "ADDING : " << tensorPinMap[*op].back()->getName() << std::endl;
        }
        if(prev_op != op) { // if we are the last in the schedule, ignore
            const auto prev_pin_vec = tensorPinMap[*prev_op];
            auto &curr_pin_vec = tensorPinMap[*op];
            for(const auto pinCand : prev_pin_vec) 
            {
                if(std::find(curr_pin_vec.begin(), curr_pin_vec.end(),
                            pinCand) == curr_pin_vec.end()) {
                    curr_pin_vec.push_back(pinCand);
                    std::cout << "ADDING : " << tensorPinMap[*op].back()->getName() << std::endl;
                }
            }
        }
        prev_op = op;
    }
}

void GraphAnalyzer::update_pin_map()
{
}

void GraphAnalyzer::print_pin_map()
{
    std::cout << "======================================================\n";
    std::cout << "      Getting Liveness Data...\n";
    std::cout << "======================================================\n";
    this->get_liveness_data();
    std::cout << "======================================================\n";
    std::cout << "      Creating Pin Map ...\n";
    std::cout << "======================================================\n";
    this->create_pin_map();
    for(auto &node : tensorPinMap)
    {
        print_pin(node);
    }
    std::cout << "======================================================\n";
    std::cout << "      Validating Pin Map ...\n";
    std::cout << "======================================================\n";
    this->validate_pin();

    for(auto &node : tensorPinMap)
    {
        print_pin(node);
    }
}

void GraphAnalyzer::create_pin_schedule()
{
    this->remove_ttl_overlaps();
    this->validate_pin();
}

void GraphAnalyzer::print_tensor_map()
{
    std::cout << "======================================================\n";
    std::cout << "      Getting Liveness Data...\n";
    std::cout << "======================================================\n";

    this->get_liveness_data();
    for(auto &node : livenessMap)
    {
        print_node(node);
    }
}

void GraphAnalyzer::dry_run_network()
{
    std::cout << "======================================================\n";
    std::cout << "      Tiling operators of the network...\n";
    std::cout << "======================================================\n";
    for (auto nameOp : network->getOperators()) {
        Operator* op = nameOp.second;
        dout(0) << "Tiling " << op->getName() << " ("
                << OpType_Name(op->getOpType()) << ").\n";
        op->tile();
    }

    std::cout << "======================================================\n";
    std::cout << "      Scheduling operators of the network...\n";
    std::cout << "======================================================\n";
    // Initialize number of pending inputs for every operator and put Data
    // operators into the ready queue.
    for (auto nameOp : network->getOperators()) {
        Operator* op = nameOp.second;
        Vertex vertex = op->getVertex();
        int numPendingInputs = boost::in_degree(vertex, network->getGraph());
        op->setNumPendingInputs(numPendingInputs);
        if (numPendingInputs == 0)
            readyQueue.push_back(op);
    }

    scheduleReady();
}

void GraphAnalyzer::remove_duplicate_cycles()
{
    for(auto &node : livenessMap)
    {
        node.second->remove_duplicate_cycles();
    }
}

} //smaug
